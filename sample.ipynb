{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1091ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661a27d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b21aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/3f/14/e105b8ef6d324e789c1589e95cb0ab63f3e07c2216d68b1178b7c21b7d2a/torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "Successfully installed torch-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9f3681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3645bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping human_body_prior as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/nghorbani/human_body_prior.git\n",
      "  Cloning https://github.com/nghorbani/human_body_prior.git to /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-anpdku31\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nghorbani/human_body_prior.git /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-anpdku31\n",
      "  Resolved https://github.com/nghorbani/human_body_prior.git to commit 4c246d8a83ce16d3cff9c79dcf04d81fa440a6bc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: human-body-prior\n",
      "  Building wheel for human-body-prior (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for human-body-prior: filename=human_body_prior-2.2.2.0-py3-none-any.whl size=7610628 sha256=e2a8e5f97ddc10326c9f1831eb016339a6e399085d1407ca333a49810ac35a77\n",
      "  Stored in directory: /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-ephem-wheel-cache-p1j3xns5/wheels/98/0b/16/e3bf193b3949bc6db7d7f2f4032fa8fd3b7fd26e76b2127584\n",
      "Successfully built human-body-prior\n",
      "Installing collected packages: human-body-prior\n",
      "Successfully installed human-body-prior-2.2.2.0\n",
      "Collecting chumpy\n",
      "  Downloading chumpy-0.70.tar.gz (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m406.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from chumpy) (1.11.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from chumpy) (1.16.0)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from scipy>=0.13.0->chumpy) (1.24.3)\n",
      "Building wheels for collected packages: chumpy\n",
      "  Building wheel for chumpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chumpy: filename=chumpy-0.70-py3-none-any.whl size=58267 sha256=cf41c01ba8a0dd767e9c6a84fa96b1c9c0c454130b42f6d55ff29ec36d95a8c6\n",
      "  Stored in directory: /Users/roawaal/Library/Caches/pip/wheels/91/96/31/3e16aa7084783b2e57a81bb9a7fa3598d32445a964b1692259\n",
      "Successfully built chumpy\n",
      "Installing collected packages: chumpy\n",
      "Successfully installed chumpy-0.70\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y human_body_prior\n",
    "!pip install git+https://github.com/nghorbani/human_body_prior.git\n",
    "!pip install chumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d850b34a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omegaconf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_vposer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mosp\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloguru\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexprdir2model\u001b[39m(expr_dir, model_cfg_override: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omegaconf'"
     ]
    }
   ],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.model_loader import load_vposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a21c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting omegaconf\n",
      "  Obtaining dependency information for omegaconf from https://files.pythonhosted.org/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from omegaconf) (6.0)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=eda963c9477c7e823300a4231bea401d37c1cb784d617c49d96731a84c4ef504\n",
      "  Stored in directory: /Users/roawaal/Library/Caches/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install omegaconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d90a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: omegaconf in /Users/roawaal/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Collecting loguru\n",
      "  Obtaining dependency information for loguru from https://files.pythonhosted.org/packages/0c/29/0348de65b8cc732daa3e33e67806420b2ae89bdce2b04af740289c5c6c8c/loguru-0.7.3-py3-none-any.whl.metadata\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: chumpy in /Users/roawaal/anaconda3/lib/python3.11/site-packages (0.70)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: scipy>=0.13.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from chumpy) (1.11.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from chumpy) (1.16.0)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from scipy>=0.13.0->chumpy) (1.24.3)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: loguru\n",
      "Successfully installed loguru-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install omegaconf loguru chumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98380526",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_vposer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)"
     ]
    }
   ],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.model_loader import load_vposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec6b711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: human-body-prior 2.2.2.0\n",
      "Uninstalling human-body-prior-2.2.2.0:\n",
      "  Successfully uninstalled human-body-prior-2.2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y human_body_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c6f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nghorbani/human_body_prior.git\n",
      "  Cloning https://github.com/nghorbani/human_body_prior.git to /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-rmagnlpf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nghorbani/human_body_prior.git /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-rmagnlpf\n",
      "  Resolved https://github.com/nghorbani/human_body_prior.git to commit 4c246d8a83ce16d3cff9c79dcf04d81fa440a6bc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: human-body-prior\n",
      "  Building wheel for human-body-prior (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for human-body-prior: filename=human_body_prior-2.2.2.0-py3-none-any.whl size=7610628 sha256=a66aea8c8e400964c4a106ab21ba493f3accc233a693ad8e0342e308ce7cb33f\n",
      "  Stored in directory: /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-ephem-wheel-cache-xyftmnbf/wheels/98/0b/16/e3bf193b3949bc6db7d7f2f4032fa8fd3b7fd26e76b2127584\n",
      "Successfully built human-body-prior\n",
      "Installing collected packages: human-body-prior\n",
      "Successfully installed human-body-prior-2.2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/nghorbani/human_body_prior.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0412b248",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_vposer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)"
     ]
    }
   ],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.model_loader import load_vposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb307117",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_vposer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_vposer' from 'human_body_prior.tools.model_loader' (/Users/roawaal/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py)"
     ]
    }
   ],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.model_loader import load_vposer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ef0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/nghorbani/human_body_prior\n",
      "  Cloning https://github.com/nghorbani/human_body_prior to /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-wpzfrhsg\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/nghorbani/human_body_prior /private/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/pip-req-build-wpzfrhsg\n",
      "  Resolved https://github.com/nghorbani/human_body_prior to commit 4c246d8a83ce16d3cff9c79dcf04d81fa440a6bc\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: omegaconf in /Users/roawaal/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from omegaconf) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from omegaconf) (6.0)\n",
      "Requirement already satisfied: loguru in /Users/roawaal/anaconda3/lib/python3.11/site-packages (0.7.3)\n",
      "Collecting trimesh\n",
      "  Obtaining dependency information for trimesh from https://files.pythonhosted.org/packages/17/f9/831a943d3a53af305560fd232e92081f8e5c067cdb9bc6733800751bf2d4/trimesh-4.6.6-py3-none-any.whl.metadata\n",
      "  Downloading trimesh-4.6.6-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from trimesh) (1.24.3)\n",
      "Downloading trimesh-4.6.6-py3-none-any.whl (709 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.3/709.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trimesh\n",
      "Successfully installed trimesh-4.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/nghorbani/human_body_prior\n",
    "!pip install omegaconf\n",
    "!pip install loguru\n",
    "!pip install trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb926f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import path as osp\n",
    "\n",
    "support_dir = '/Documents/586Project/VPoserModelFiles'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c128d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Documents/586Project/VPoserModelFiles/vposer_v2_05/\n",
      "/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz\n",
      "/Documents/586Project/VPoserModelFiles/amass_sample.npz\n"
     ]
    }
   ],
   "source": [
    "expr_dir = osp.join(support_dir,'vposer_v2_05/') #'TRAINED_MODEL_DIRECTORY'\n",
    "bm_fname =  osp.join(support_dir,'smplx_neutral_model.npz')    #'PATH_TO_SMPLX_model.npz'  neutral smpl body model\n",
    "sample_amass_fname = osp.join(support_dir, 'amass_sample.npz')  # a sample npz file from AMASS\n",
    "\n",
    "\n",
    "print(expr_dir)\n",
    "print(bm_fname)\n",
    "print(sample_amass_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b410d76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice is\u001b[39m\u001b[38;5;124m'\u001b[39m, device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[0;32m---> 10\u001b[0m bm \u001b[38;5;241m=\u001b[39m BodyModel(bm_fname\u001b[38;5;241m=\u001b[39mbm_fname)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/body_model/body_model.py:59\u001b[0m, in \u001b[0;36mBodyModel.__init__\u001b[0;34m(self, bm_fname, num_betas, num_dmpls, dmpl_fname, num_expressions, use_posedirs, model_type, dtype, persistant_buffer)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# -- Load SMPL params --\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bm_fname\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     smpl_dict \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(bm_fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbm_fname must be a .npz file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbm_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz'"
     ]
    }
   ],
   "source": [
    "#Loading SMPLx Body Model\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Choose the device to run the body model on, cuda or cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is', device)\n",
    "\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "bm = BodyModel(bm_fname=bm_fname).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9b3dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Model file not found at: /Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(bm_fname), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel file not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbm_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbody_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BodyModel\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Set device\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Model file not found at: /Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "assert os.path.isfile(bm_fname), f\"Model file not found at: {bm_fname}\"\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is', device)\n",
    "\n",
    "# Define the path to the SMPL model (adjust this to match your folder structure)\n",
    "# bm_fname = '/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz'\n",
    "bm_fname = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz')\n",
    "\n",
    "# Load the body model\n",
    "bm = BodyModel(bm_fname=bm_fname).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8aa4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is', device)\n",
    "\n",
    "# Correct path to your SMPL model file\n",
    "bm_fname = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz')\n",
    "\n",
    "# Confirm the file exists\n",
    "assert os.path.isfile(bm_fname), f\"Model file not found at: {bm_fname}\"\n",
    "\n",
    "# Load the body model\n",
    "bm = BodyModel(bm_fname=bm_fname).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55f79a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find the experiment directory: /Documents/586Project/VPoserModelFiles/vposer_v2_05/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuman_body_prior\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvposer_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VPoser\n\u001b[0;32m----> 5\u001b[0m vp, ps \u001b[38;5;241m=\u001b[39m load_model(expr_dir, model_code\u001b[38;5;241m=\u001b[39mVPoser,\n\u001b[1;32m      6\u001b[0m                               remove_words_in_model_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvp_model.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m                               disable_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                               comp_device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m vp \u001b[38;5;241m=\u001b[39m vp\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py:66\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(expr_dir, model_code, remove_words_in_model_weights, load_only_cfg, disable_grad, model_cfg_override, comp_device)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m:param expr_dir:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m model_cfg, trained_weights_fname \u001b[38;5;241m=\u001b[39m exprdir2model(expr_dir, model_cfg_override\u001b[38;5;241m=\u001b[39mmodel_cfg_override)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_only_cfg: \u001b[38;5;28;01mreturn\u001b[39;00m model_cfg\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m model_code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_code should be provided\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/tools/model_loader.py:30\u001b[0m, in \u001b[0;36mexprdir2model\u001b[0;34m(expr_dir, model_cfg_override)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexprdir2model\u001b[39m(expr_dir, model_cfg_override: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(expr_dir): \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find the experiment directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpr_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m     model_snapshots_dir \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(expr_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnapshots\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m     available_ckpts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(osp\u001b[38;5;241m.\u001b[39mjoin(model_snapshots_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)), key\u001b[38;5;241m=\u001b[39mosp\u001b[38;5;241m.\u001b[39mgetmtime)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find the experiment directory: /Documents/586Project/VPoserModelFiles/vposer_v2_05/"
     ]
    }
   ],
   "source": [
    "#Loading VPoser VAE Body Pose Prior\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                              remove_words_in_model_weights='vp_model.',\n",
    "                              disable_grad=True,\n",
    "                              comp_device=device)\n",
    "vp = vp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea48af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "expr_dir = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/vposer_v2_05')\n",
    "assert os.path.isdir(expr_dir), f\"VPoser model directory not found: {expr_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9c0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 17:12:58.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mNo GPU detected. Loading on CPU!\u001b[0m\n",
      "\u001b[32m2025-04-12 17:12:58.348\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mLoaded model in eval mode with trained weights: /Users/roawaal/Documents/586Project/VPoserModelFiles/vposer_v2_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                    remove_words_in_model_weights='vp_model.',\n",
    "                    disable_grad=True,\n",
    "                    comp_device=device)\n",
    "vp = vp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdf85092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 17:14:15.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mNo GPU detected. Loading on CPU!\u001b[0m\n",
      "\u001b[32m2025-04-12 17:14:15.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mLoaded model in eval mode with trained weights: /Users/roawaal/Documents/586Project/VPoserModelFiles/vposer_v2_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "VPoser model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define and verify path to VPoser model directory\n",
    "expr_dir = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/vposer_v2_05')\n",
    "assert os.path.isdir(expr_dir), f\"VPoser model directory not found: {expr_dir}\"\n",
    "\n",
    "# Load VPoser model\n",
    "vp, ps = load_model(expr_dir,\n",
    "                    model_code=VPoser,\n",
    "                    remove_words_in_model_weights='vp_model.',\n",
    "                    disable_grad=True,\n",
    "                    comp_device=device)\n",
    "vp = vp.to(device)\n",
    "print(\"VPoser model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a98ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Documents/586Project/VPoserModelFiles/amass_sample.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the body poses from amass sample file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(sample_amass_fname)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m66\u001b[39m]\n\u001b[1;32m      4\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(amass_body_pose)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamass_body_pose.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, amass_body_pose\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Documents/586Project/VPoserModelFiles/amass_sample.npz'"
     ]
    }
   ],
   "source": [
    "# Prepare the body poses from amass sample file\n",
    "#  indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\n",
    "amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n",
    "amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n",
    "print('amass_body_pose.shape', amass_body_pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513af2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cpu\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "SMPL body model file not found at: /Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m sample_amass_fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Documents/586Project/AMASS_CMUsubset/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with actual path\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Assert model file exists\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(bm_fname), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSMPL body model file not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbm_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(expr_dir), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVPoser model directory not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpr_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(sample_amass_fname), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMASS sample file not found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_amass_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: SMPL body model file not found at: /Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "\n",
    "# Set the device to CPU or CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device is', device)\n",
    "\n",
    "# Paths for body model and VPoser model\n",
    "\n",
    "bm_fname = '/Documents/586Project/VPoserModelFiles/smplx_neutral_model.npz'  # Replace with actual path\n",
    "expr_dir = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/vposer_v2_05') # Replace with actual path\n",
    "sample_amass_fname = '/Documents/586Project/AMASS_CMUsubset/'  # Replace with actual path\n",
    "\n",
    "# Assert model file exists\n",
    "assert os.path.isfile(bm_fname), f\"SMPL body model file not found at: {bm_fname}\"\n",
    "assert os.path.isdir(expr_dir), f\"VPoser model directory not found at: {expr_dir}\"\n",
    "assert os.path.isfile(sample_amass_fname), f\"AMASS sample file not found at: {sample_amass_fname}\"\n",
    "\n",
    "# Load the SMPLx Body Model\n",
    "bm = BodyModel(bm_fname=bm_fname).to(device)\n",
    "\n",
    "# Load the VPoser VAE Body Pose Prior\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                    remove_words_in_model_weights='vp_model.',\n",
    "                    disable_grad=True,\n",
    "                    comp_device=device)\n",
    "vp = vp.to(device)\n",
    "\n",
    "# Prepare the body poses from AMASS sample file\n",
    "# Indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\n",
    "amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n",
    "amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n",
    "print('amass_body_pose.shape', amass_body_pose.shape)\n",
    "\n",
    "# Visualizing the body pose (optional)\n",
    "def visualize_3d_pose(body_pose):\n",
    "    # Assuming body_pose is a tensor with shape [batch_size, num_joints, 3]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # You would need to map body_pose to joint coordinates, here's a placeholder plot\n",
    "    # Visualizing just the 3D joints (as an example)\n",
    "    ax.scatter(body_pose[:, 0].cpu().numpy(),  # X\n",
    "               body_pose[:, 1].cpu().numpy(),  # Y\n",
    "               body_pose[:, 2].cpu().numpy(),  # Z\n",
    "               color='b')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first pose from the AMASS data (for example)\n",
    "visualize_3d_pose(amass_body_pose[0])\n",
    "\n",
    "# Use the VPoser model to get latent vectors for the body pose\n",
    "latent_body_pose = vp.encode(amass_body_pose)\n",
    "\n",
    "# Decoding back to body pose space using VPoser\n",
    "decoded_body_pose = vp.decode(latent_body_pose)\n",
    "\n",
    "# Visualize the decoded pose\n",
    "visualize_3d_pose(decoded_body_pose[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18864907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available .npz files: ['06_09_poses.npz', '01_09_poses.npz', '13_28_poses.npz', '13_09_poses.npz', '05_07_poses.npz', '13_31_poses.npz', '06_04_poses.npz', '02_07_poses.npz', '01_10_poses.npz', '13_25_poses.npz', '06_10_poses.npz', '05_13_poses.npz', '13_04_poses.npz', '12_02_poses.npz', '13_10_poses.npz', '13_40_poses.npz', '05_18_poses.npz', '13_02_poses.npz', '12_04_poses.npz', '05_20_poses.npz', '13_16_poses.npz', '02_01_poses.npz', '01_02_poses.npz', '05_01_poses.npz', '13_37_poses.npz', '13_23_poses.npz', '05_15_poses.npz', '13_29_poses.npz', '06_08_poses.npz', '01_08_poses.npz', '13_08_poses.npz', '01_11_poses.npz', '06_11_poses.npz', '13_24_poses.npz', '05_12_poses.npz', '05_06_poses.npz', '06_05_poses.npz', '13_30_poses.npz', '02_06_poses.npz', '01_05_poses.npz', '13_11_poses.npz', '13_05_poses.npz', '12_03_poses.npz', '05_19_poses.npz', '13_41_poses.npz', '13_17_poses.npz', '13_03_poses.npz', '13_22_poses.npz', '05_14_poses.npz', '01_03_poses.npz', '13_36_poses.npz', '06_03_poses.npz', '05_08_poses.npz', '02_08_poses.npz', '13_06_poses.npz', '13_12_poses.npz', '13_33_poses.npz', '06_06_poses.npz', '05_05_poses.npz', '01_06_poses.npz', '02_05_poses.npz', '05_11_poses.npz', '13_27_poses.npz', '06_12_poses.npz', '13_42_poses.npz', '13_38_poses.npz', '13_19_poses.npz', '02_03_poses.npz', '13_35_poses.npz', '05_03_poses.npz', '05_17_poses.npz', '06_14_poses.npz', '13_21_poses.npz', '13_14_poses.npz', '05_09_poses.npz', '02_09_poses.npz', '13_13_poses.npz', '12_01_poses.npz', '13_07_poses.npz', '02_10_poses.npz', '05_10_poses.npz', '06_13_poses.npz', '13_26_poses.npz', '06_07_poses.npz', '13_32_poses.npz', '05_04_poses.npz', '01_07_poses.npz', '02_04_poses.npz', '13_39_poses.npz', '13_18_poses.npz', '05_16_poses.npz', '13_20_poses.npz', '06_15_poses.npz', '01_01_poses.npz', '02_02_poses.npz', '13_34_poses.npz', '06_01_poses.npz', '05_02_poses.npz', '13_15_poses.npz', '13_01_poses.npz']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')  # Update with the correct directory path\n",
    "amass_files = [f for f in os.listdir(amass_dir) if f.endswith('.npz')]\n",
    "\n",
    "print(\"Available .npz files:\", amass_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df87c32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 17:39:36.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mNo GPU detected. Loading on CPU!\u001b[0m\n",
      "\u001b[32m2025-04-12 17:39:36.653\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mLoaded model in eval mode with trained weights: /Users/roawaal/Documents/586Project/VPoserModelFiles/vposer_v2_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Normal' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m encode_poses_to_latent_space(npz_files, amass_dir, device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Example of the shape of the latent pose (depends on VPoser configuration)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatent poses shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[latent\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlatent\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlatent_poses]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 56\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m encode_poses_to_latent_space(npz_files, amass_dir, device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Example of the shape of the latent pose (depends on VPoser configuration)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatent poses shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[latent\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mlatent\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mlatent_poses]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Normal' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Path to the VPoser model directory (update with your actual path)\n",
    "expr_dir = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/vposer_v2_05')  # Replace with actual path\n",
    "\n",
    "# Load VPoser model\n",
    "vp, ps = load_model(expr_dir, model_code=VPoser,\n",
    "                    remove_words_in_model_weights='vp_model.',\n",
    "                    disable_grad=True,\n",
    "                    comp_device=device)\n",
    "vp = vp.to(device)\n",
    "\n",
    "# Path to AMASS CMU subset directory (update with your actual path)\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')   # Replace with actual path\n",
    "\n",
    "# Get all .npz files in the AMASS directory\n",
    "npz_files = [f for f in os.listdir(amass_dir) if f.endswith('.npz')]\n",
    "npz_files.sort()  # Sort files for consistent order\n",
    "\n",
    "# Function to encode SMPL poses into latent space using VPoser\n",
    "def encode_poses_to_latent_space(npz_files, amass_dir, device):\n",
    "    latent_poses = []\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        # Load the .npz file\n",
    "        npz_path = os.path.join(amass_dir, npz_file)\n",
    "        amass_data = np.load(npz_path)\n",
    "        \n",
    "        # Extract body poses (3:66 to remove global rotation, hands/fingers, etc.)\n",
    "        body_pose = amass_data['poses'][:, 3:66]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        body_pose = torch.from_numpy(body_pose).float().to(device)\n",
    "        \n",
    "        # Encode the body pose using VPoser (latent space)\n",
    "        with torch.no_grad():\n",
    "            latent_vec = vp.encode(body_pose)  # Encoding to latent space\n",
    "\n",
    "        # Store latent vector for later use\n",
    "        latent_poses.append(latent_vec)\n",
    "    \n",
    "    return latent_poses\n",
    "\n",
    "# Encode poses from all files into latent space\n",
    "latent_poses = encode_poses_to_latent_space(npz_files, amass_dir, device)\n",
    "\n",
    "# Example of the shape of the latent pose (depends on VPoser configuration)\n",
    "print(f\"Latent poses shape: {[latent.shape for latent in latent_poses]}\")\n",
    "\n",
    "# Now, latent_poses can be used for training an autoregressive model (e.g., Transformer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe6a5e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent poses shape: [torch.Size([2751, 32]), torch.Size([4346, 32]), torch.Size([4510, 32]), torch.Size([4376, 32]), torch.Size([5133, 32]), torch.Size([4839, 32]), torch.Size([2610, 32]), torch.Size([4242, 32]), torch.Size([3266, 32]), torch.Size([4654, 32]), torch.Size([343, 32]), torch.Size([298, 32]), torch.Size([173, 32]), torch.Size([483, 32]), torch.Size([1854, 32]), torch.Size([2235, 32]), torch.Size([2251, 32]), torch.Size([1500, 32]), torch.Size([1033, 32]), torch.Size([2645, 32]), torch.Size([598, 32]), torch.Size([1123, 32]), torch.Size([434, 32]), torch.Size([1199, 32]), torch.Size([915, 32]), torch.Size([885, 32]), torch.Size([1191, 32]), torch.Size([721, 32]), torch.Size([1143, 32]), torch.Size([817, 32]), torch.Size([591, 32]), torch.Size([1354, 32]), torch.Size([1095, 32]), torch.Size([642, 32]), torch.Size([540, 32]), torch.Size([525, 32]), torch.Size([1043, 32]), torch.Size([1829, 32]), torch.Size([860, 32]), torch.Size([1095, 32]), torch.Size([494, 32]), torch.Size([527, 32]), torch.Size([396, 32]), torch.Size([385, 32]), torch.Size([402, 32]), torch.Size([576, 32]), torch.Size([342, 32]), torch.Size([301, 32]), torch.Size([1113, 32]), torch.Size([1146, 32]), torch.Size([1222, 32]), torch.Size([4905, 32]), torch.Size([479, 32]), torch.Size([545, 32]), torch.Size([523, 32]), torch.Size([673, 32]), torch.Size([565, 32]), torch.Size([17799, 32]), torch.Size([2313, 32]), torch.Size([3477, 32]), torch.Size([3715, 32]), torch.Size([4759, 32]), torch.Size([4403, 32]), torch.Size([4413, 32]), torch.Size([1452, 32]), torch.Size([1603, 32]), torch.Size([1102, 32]), torch.Size([2396, 32]), torch.Size([415, 32]), torch.Size([3434, 32]), torch.Size([439, 32]), torch.Size([2424, 32]), torch.Size([2382, 32]), torch.Size([2275, 32]), torch.Size([4840, 32]), torch.Size([3000, 32]), torch.Size([426, 32]), torch.Size([5719, 32]), torch.Size([5439, 32]), torch.Size([4962, 32]), torch.Size([4017, 32]), torch.Size([4034, 32]), torch.Size([4009, 32]), torch.Size([3028, 32]), torch.Size([4169, 32]), torch.Size([4121, 32]), torch.Size([4592, 32]), torch.Size([2465, 32]), torch.Size([3017, 32]), torch.Size([336, 32]), torch.Size([1499, 32]), torch.Size([1417, 32]), torch.Size([1201, 32]), torch.Size([1379, 32]), torch.Size([1478, 32]), torch.Size([1244, 32]), torch.Size([351, 32]), torch.Size([319, 32]), torch.Size([369, 32]), torch.Size([400, 32])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'latent_poses_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save the latent poses into a .npz file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Documents/586Project/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m np\u001b[38;5;241m.\u001b[39msavez(output_file, latent_poses\u001b[38;5;241m=\u001b[39mlatent_poses_array)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'latent_poses_array' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to encode SMPL poses into latent space using VPoser\n",
    "def encode_poses_to_latent_space(npz_files, amass_dir, device):\n",
    "    latent_poses = []\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        # Load the .npz file\n",
    "        npz_path = os.path.join(amass_dir, npz_file)\n",
    "        amass_data = np.load(npz_path)\n",
    "        \n",
    "        # Extract body poses (3:66 to remove global rotation, hands/fingers, etc.)\n",
    "        body_pose = amass_data['poses'][:, 3:66]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        body_pose = torch.from_numpy(body_pose).float().to(device)\n",
    "        \n",
    "        # Encode the body pose using VPoser (latent space)\n",
    "        with torch.no_grad():\n",
    "            latent_dist = vp.encode(body_pose)  # This is a distribution (Normal)\n",
    "        \n",
    "        # Extract the mean of the distribution (latent vector)\n",
    "        latent_vec = latent_dist.mean  # Use the mean (or rsample for sampling)\n",
    "        \n",
    "        # Store latent vector for later use\n",
    "        latent_poses.append(latent_vec)\n",
    "    \n",
    "    return latent_poses\n",
    "\n",
    "# Encode poses from all files into latent space\n",
    "latent_poses = encode_poses_to_latent_space(npz_files, amass_dir, device)\n",
    "\n",
    "# Example of the shape of the latent pose (depends on VPoser configuration)\n",
    "print(f\"Latent poses shape: {[latent.shape for latent in latent_poses]}\")\n",
    "# Save the latent poses into a .npz file\n",
    "output_file = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "np.savez(output_file, latent_poses=latent_poses_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "624b55bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m encode_poses_to_latent_space(npz_files, amass_dir, device)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Convert the list of latent vectors to a numpy array (for saving)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m latent_poses_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(latent_poses)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save the latent poses into a .npz file\u001b[39;00m\n\u001b[1;32m     41\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/shape_base.py:464\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    462\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    466\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    467\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "#The error you're encountering is because the latent object is likely an instance of the Normal class from the torch.distributions module, which represents a distribution and doesn't directly have the shape attribute in the way that a tensor does.\n",
    "\n",
    "#VPoser typically returns a distribution (such as Normal for the latent space), rather than a simple tensor. You'll need to access the mean or rsample() method of this distribution to get the actual latent vector.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Function to encode SMPL poses into latent space using VPoser\n",
    "def encode_poses_to_latent_space(npz_files, amass_dir, device):\n",
    "    latent_poses = []\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        # Load the .npz file\n",
    "        npz_path = os.path.join(amass_dir, npz_file)\n",
    "        amass_data = np.load(npz_path)\n",
    "        \n",
    "        # Extract body poses (3:66 to remove global rotation, hands/fingers, etc.)\n",
    "        body_pose = amass_data['poses'][:, 3:66]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        body_pose = torch.from_numpy(body_pose).float().to(device)\n",
    "        \n",
    "        # Encode the body pose using VPoser (latent space)\n",
    "        with torch.no_grad():\n",
    "            latent_dist = vp.encode(body_pose)  # This is a distribution (Normal)\n",
    "        \n",
    "        # Extract the mean of the distribution (latent vector)\n",
    "        latent_vec = latent_dist.mean  # Use the mean (or rsample for sampling)\n",
    "        \n",
    "        # Store latent vector for later use\n",
    "        latent_poses.append(latent_vec.cpu().numpy())  # Convert to numpy for saving\n",
    "    \n",
    "    return latent_poses\n",
    "\n",
    "# Encode poses from all files into latent space\n",
    "latent_poses = encode_poses_to_latent_space(npz_files, amass_dir, device)\n",
    "\n",
    "# Convert the list of latent vectors to a numpy array (for saving)\n",
    "latent_poses_array = np.stack(latent_poses)\n",
    "\n",
    "# Save the latent poses into a .npz file\n",
    "output_file = '/latent_poses.npz'\n",
    "np.savez(output_file, latent_poses=latent_poses_array)\n",
    "\n",
    "print(f\"Latent poses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f72b3ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_09_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_09_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_28_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_09_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_07_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_31_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_04_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_07_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_10_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_25_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_10_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_13_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_04_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/12_02_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_10_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_40_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_18_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_02_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/12_04_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_20_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_16_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_01_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_02_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_01_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_37_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_23_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_15_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_29_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_08_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_08_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_08_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_11_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_11_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_24_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_12_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_06_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_05_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_30_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_06_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_05_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_11_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_05_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/12_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_19_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_41_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_17_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_22_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_14_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_36_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_08_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_08_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_06_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_12_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_33_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_06_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_05_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_06_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_05_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_11_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_27_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_12_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_42_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_38_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_19_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_35_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_03_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_17_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_14_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_21_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_14_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_09_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_09_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_13_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/12_01_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_07_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_10_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_10_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_13_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_26_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_07_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_32_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_04_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_07_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_04_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_39_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_18_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_16_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_20_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_15_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/01_01_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/02_02_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_34_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/06_01_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/05_02_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_15_poses.npz\n",
      "Processing file: /Users/roawaal/Documents/586Project/AMASS_CMUsubset/13_01_poses.npz\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m encode_poses_to_latent_space(npz_files, amass_dir, device)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Convert the list of latent vectors to a numpy array (for saving)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m latent_poses_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(latent_poses)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Save the latent poses into a .npz file\u001b[39;00m\n\u001b[1;32m     53\u001b[0m output_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Documents/586Project/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/shape_base.py:464\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[1;32m    462\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    466\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    467\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function to encode SMPL poses into latent space using VPoser\n",
    "def encode_poses_to_latent_space(npz_files, amass_dir, device):\n",
    "    latent_poses = []\n",
    "    \n",
    "    for npz_file in npz_files:\n",
    "        # Load the .npz file\n",
    "        npz_path = os.path.join(amass_dir, npz_file)\n",
    "        print(f\"Processing file: {npz_path}\")  # Debugging line to see which file is being processed\n",
    "        amass_data = np.load(npz_path)\n",
    "        \n",
    "        # Extract body poses (3:66 to remove global rotation, hands/fingers, etc.)\n",
    "        body_pose = amass_data['poses'][:, 3:66]\n",
    "        \n",
    "        # Convert to torch tensor\n",
    "        body_pose = torch.from_numpy(body_pose).float().to(device)\n",
    "        \n",
    "        # Encode the body pose using VPoser (latent space)\n",
    "        with torch.no_grad():\n",
    "            latent_dist = vp.encode(body_pose)  # This is a distribution (Normal)\n",
    "        \n",
    "        # Extract the mean of the distribution (latent vector)\n",
    "        latent_vec = latent_dist.mean  # Use the mean (or rsample for sampling)\n",
    "        \n",
    "        # Store latent vector for later use\n",
    "        latent_poses.append(latent_vec.cpu().numpy())  # Convert to numpy for saving\n",
    "    \n",
    "    if len(latent_poses) == 0:\n",
    "        raise ValueError(\"No latent poses were encoded, check the input data and processing pipeline.\")\n",
    "    \n",
    "    return latent_poses\n",
    "\n",
    "# List of npz files in your AMASS dataset directory\n",
    "npz_files = ['06_09_poses.npz', '01_09_poses.npz', '13_28_poses.npz', '13_09_poses.npz', '05_07_poses.npz', '13_31_poses.npz', '06_04_poses.npz', '02_07_poses.npz', '01_10_poses.npz', '13_25_poses.npz', '06_10_poses.npz', '05_13_poses.npz', '13_04_poses.npz', '12_02_poses.npz', '13_10_poses.npz', '13_40_poses.npz', '05_18_poses.npz', '13_02_poses.npz', '12_04_poses.npz', '05_20_poses.npz', '13_16_poses.npz', '02_01_poses.npz', '01_02_poses.npz', '05_01_poses.npz', '13_37_poses.npz', '13_23_poses.npz', '05_15_poses.npz', '13_29_poses.npz', '06_08_poses.npz', '01_08_poses.npz', '13_08_poses.npz', '01_11_poses.npz', '06_11_poses.npz', '13_24_poses.npz', '05_12_poses.npz', '05_06_poses.npz', '06_05_poses.npz', '13_30_poses.npz', '02_06_poses.npz', '01_05_poses.npz', '13_11_poses.npz', '13_05_poses.npz', '12_03_poses.npz', '05_19_poses.npz', '13_41_poses.npz', '13_17_poses.npz', '13_03_poses.npz', '13_22_poses.npz', '05_14_poses.npz', '01_03_poses.npz', '13_36_poses.npz', '06_03_poses.npz', '05_08_poses.npz', '02_08_poses.npz', '13_06_poses.npz', '13_12_poses.npz', '13_33_poses.npz', '06_06_poses.npz', '05_05_poses.npz', '01_06_poses.npz', '02_05_poses.npz', '05_11_poses.npz', '13_27_poses.npz', '06_12_poses.npz', '13_42_poses.npz', '13_38_poses.npz', '13_19_poses.npz', '02_03_poses.npz', '13_35_poses.npz', '05_03_poses.npz', '05_17_poses.npz', '06_14_poses.npz', '13_21_poses.npz', '13_14_poses.npz', '05_09_poses.npz', '02_09_poses.npz', '13_13_poses.npz', '12_01_poses.npz', '13_07_poses.npz', '02_10_poses.npz', '05_10_poses.npz', '06_13_poses.npz', '13_26_poses.npz', '06_07_poses.npz', '13_32_poses.npz', '05_04_poses.npz', '01_07_poses.npz', '02_04_poses.npz', '13_39_poses.npz', '13_18_poses.npz', '05_16_poses.npz', '13_20_poses.npz', '06_15_poses.npz', '01_01_poses.npz', '02_02_poses.npz', '13_34_poses.npz', '06_01_poses.npz', '05_02_poses.npz', '13_15_poses.npz', '13_01_poses.npz']  # Add all the .npz filenames here\n",
    "\n",
    "# AMASS directory path\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')   # Update this path to where your AMASS dataset is stored\n",
    "\n",
    "# Ensure that your device is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Encode poses from all files into latent space\n",
    "latent_poses = encode_poses_to_latent_space(npz_files, amass_dir, device)\n",
    "\n",
    "# Convert the list of latent vectors to a numpy array (for saving)\n",
    "latent_poses_array = np.stack(latent_poses)\n",
    "\n",
    "# Save the latent poses into a .npz file\n",
    "output_file = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "np.savez(output_file, latent_poses=latent_poses_array)\n",
    "\n",
    "print(f\"Latent poses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62f3771e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Documents/586Project/AMASS_CMUsubset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the body poses from amass sample file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#  indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(sample_amass_fname)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m66\u001b[39m]\n\u001b[1;32m      4\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(amass_body_pose)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamass_body_pose.shape\u001b[39m\u001b[38;5;124m'\u001b[39m, amass_body_pose\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Documents/586Project/AMASS_CMUsubset/'"
     ]
    }
   ],
   "source": [
    "# Prepare the body poses from amass sample file\n",
    "#  indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\n",
    "amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n",
    "amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n",
    "print('amass_body_pose.shape', amass_body_pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a16b66d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m sample_amass_fname \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_09_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_09_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_28_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_09_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_07_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_31_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_04_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_07_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_10_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_25_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_10_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_13_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_04_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12_02_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_10_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_40_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_18_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_02_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12_04_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_20_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_16_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_02_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_37_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_23_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_15_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_29_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_08_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_08_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_08_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_11_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_11_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_24_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_12_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_06_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_05_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_30_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_06_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_05_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_11_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_05_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_19_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_41_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_17_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_22_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_14_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_36_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_08_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_08_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_06_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_12_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_33_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_06_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_05_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_06_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_05_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_11_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_27_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_12_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_42_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_38_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_19_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_35_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_03_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_17_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_14_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_21_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_14_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_09_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_09_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_13_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_07_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_10_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_10_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_13_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_26_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_07_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_32_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_04_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_07_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_04_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_39_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_18_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_16_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_20_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_15_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02_02_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_34_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05_02_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_15_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m13_01_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the poses from the .npz file, excluding global rotation, hands, and fingers\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(sample_amass_fname)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m'\u001b[39m][:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m66\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert to a PyTorch tensor and move to the correct device (GPU or CPU)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(amass_body_pose)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# Load the sample AMASS file (adjust the filename if needed)\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')  \n",
    "sample_amass_fname = ['06_09_poses.npz', '01_09_poses.npz', '13_28_poses.npz', '13_09_poses.npz', '05_07_poses.npz', '13_31_poses.npz', '06_04_poses.npz', '02_07_poses.npz', '01_10_poses.npz', '13_25_poses.npz', '06_10_poses.npz', '05_13_poses.npz', '13_04_poses.npz', '12_02_poses.npz', '13_10_poses.npz', '13_40_poses.npz', '05_18_poses.npz', '13_02_poses.npz', '12_04_poses.npz', '05_20_poses.npz', '13_16_poses.npz', '02_01_poses.npz', '01_02_poses.npz', '05_01_poses.npz', '13_37_poses.npz', '13_23_poses.npz', '05_15_poses.npz', '13_29_poses.npz', '06_08_poses.npz', '01_08_poses.npz', '13_08_poses.npz', '01_11_poses.npz', '06_11_poses.npz', '13_24_poses.npz', '05_12_poses.npz', '05_06_poses.npz', '06_05_poses.npz', '13_30_poses.npz', '02_06_poses.npz', '01_05_poses.npz', '13_11_poses.npz', '13_05_poses.npz', '12_03_poses.npz', '05_19_poses.npz', '13_41_poses.npz', '13_17_poses.npz', '13_03_poses.npz', '13_22_poses.npz', '05_14_poses.npz', '01_03_poses.npz', '13_36_poses.npz', '06_03_poses.npz', '05_08_poses.npz', '02_08_poses.npz', '13_06_poses.npz', '13_12_poses.npz', '13_33_poses.npz', '06_06_poses.npz', '05_05_poses.npz', '01_06_poses.npz', '02_05_poses.npz', '05_11_poses.npz', '13_27_poses.npz', '06_12_poses.npz', '13_42_poses.npz', '13_38_poses.npz', '13_19_poses.npz', '02_03_poses.npz', '13_35_poses.npz', '05_03_poses.npz', '05_17_poses.npz', '06_14_poses.npz', '13_21_poses.npz', '13_14_poses.npz', '05_09_poses.npz', '02_09_poses.npz', '13_13_poses.npz', '12_01_poses.npz', '13_07_poses.npz', '02_10_poses.npz', '05_10_poses.npz', '06_13_poses.npz', '13_26_poses.npz', '06_07_poses.npz', '13_32_poses.npz', '05_04_poses.npz', '01_07_poses.npz', '02_04_poses.npz', '13_39_poses.npz', '13_18_poses.npz', '05_16_poses.npz', '13_20_poses.npz', '06_15_poses.npz', '01_01_poses.npz', '02_02_poses.npz', '13_34_poses.npz', '06_01_poses.npz', '05_02_poses.npz', '13_15_poses.npz', '13_01_poses.npz']\n",
    "# Load the poses from the .npz file, excluding global rotation, hands, and fingers\n",
    "amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n",
    "\n",
    "# Convert to a PyTorch tensor and move to the correct device (GPU or CPU)\n",
    "amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n",
    "\n",
    "# Print the shape of the body pose to check the dimensions\n",
    "print('amass_body_pose.shape', amass_body_pose.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f69b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 06_09_poses.npz, shape: torch.Size([301, 63])\n",
      "Loaded 01_09_poses.npz, shape: torch.Size([4242, 63])\n",
      "Loaded 13_28_poses.npz, shape: torch.Size([4121, 63])\n",
      "Loaded 13_09_poses.npz, shape: torch.Size([1102, 63])\n",
      "Loaded 05_07_poses.npz, shape: torch.Size([1191, 63])\n",
      "Loaded 13_31_poses.npz, shape: torch.Size([3017, 63])\n",
      "Loaded 06_04_poses.npz, shape: torch.Size([396, 63])\n",
      "Loaded 02_07_poses.npz, shape: torch.Size([2251, 63])\n",
      "Loaded 01_10_poses.npz, shape: torch.Size([3266, 63])\n",
      "Loaded 13_25_poses.npz, shape: torch.Size([4009, 63])\n",
      "Loaded 06_10_poses.npz, shape: torch.Size([1113, 63])\n",
      "Loaded 05_13_poses.npz, shape: torch.Size([1095, 63])\n",
      "Loaded 13_04_poses.npz, shape: torch.Size([4759, 63])\n",
      "Loaded 12_02_poses.npz, shape: torch.Size([673, 63])\n",
      "Loaded 13_10_poses.npz, shape: torch.Size([2396, 63])\n",
      "Loaded 13_40_poses.npz, shape: torch.Size([319, 63])\n",
      "Loaded 05_18_poses.npz, shape: torch.Size([1829, 63])\n",
      "Loaded 13_02_poses.npz, shape: torch.Size([3477, 63])\n",
      "Loaded 12_04_poses.npz, shape: torch.Size([17799, 63])\n",
      "Loaded 05_20_poses.npz, shape: torch.Size([1095, 63])\n",
      "Loaded 13_16_poses.npz, shape: torch.Size([2275, 63])\n",
      "Loaded 02_01_poses.npz, shape: torch.Size([343, 63])\n",
      "Loaded 01_02_poses.npz, shape: torch.Size([4346, 63])\n",
      "Loaded 05_01_poses.npz, shape: torch.Size([598, 63])\n",
      "Loaded 13_37_poses.npz, shape: torch.Size([1478, 63])\n",
      "Loaded 13_23_poses.npz, shape: torch.Size([4017, 63])\n",
      "Loaded 05_15_poses.npz, shape: torch.Size([540, 63])\n",
      "Loaded 13_29_poses.npz, shape: torch.Size([4592, 63])\n",
      "Loaded 06_08_poses.npz, shape: torch.Size([342, 63])\n",
      "Loaded 01_08_poses.npz, shape: torch.Size([2610, 63])\n",
      "Loaded 13_08_poses.npz, shape: torch.Size([1603, 63])\n",
      "Loaded 01_11_poses.npz, shape: torch.Size([4654, 63])\n",
      "Loaded 06_11_poses.npz, shape: torch.Size([1146, 63])\n",
      "Loaded 13_24_poses.npz, shape: torch.Size([4034, 63])\n",
      "Loaded 05_12_poses.npz, shape: torch.Size([1354, 63])\n",
      "Loaded 05_06_poses.npz, shape: torch.Size([885, 63])\n",
      "Loaded 06_05_poses.npz, shape: torch.Size([385, 63])\n",
      "Loaded 13_30_poses.npz, shape: torch.Size([2465, 63])\n",
      "Loaded 02_06_poses.npz, shape: torch.Size([2235, 63])\n",
      "Loaded 01_05_poses.npz, shape: torch.Size([4376, 63])\n",
      "Loaded 13_11_poses.npz, shape: torch.Size([415, 63])\n",
      "Loaded 13_05_poses.npz, shape: torch.Size([4403, 63])\n",
      "Loaded 12_03_poses.npz, shape: torch.Size([565, 63])\n",
      "Loaded 05_19_poses.npz, shape: torch.Size([860, 63])\n",
      "Loaded 13_41_poses.npz, shape: torch.Size([369, 63])\n",
      "Loaded 13_17_poses.npz, shape: torch.Size([4840, 63])\n",
      "Loaded 13_03_poses.npz, shape: torch.Size([3715, 63])\n",
      "Loaded 13_22_poses.npz, shape: torch.Size([4962, 63])\n",
      "Loaded 05_14_poses.npz, shape: torch.Size([642, 63])\n",
      "Loaded 01_03_poses.npz, shape: torch.Size([4510, 63])\n",
      "Loaded 13_36_poses.npz, shape: torch.Size([1379, 63])\n",
      "Loaded 06_03_poses.npz, shape: torch.Size([527, 63])\n",
      "Loaded 05_08_poses.npz, shape: torch.Size([721, 63])\n",
      "Loaded 02_08_poses.npz, shape: torch.Size([1500, 63])\n",
      "Loaded 13_06_poses.npz, shape: torch.Size([4413, 63])\n",
      "Loaded 13_12_poses.npz, shape: torch.Size([3434, 63])\n",
      "Loaded 13_33_poses.npz, shape: torch.Size([1499, 63])\n",
      "Loaded 06_06_poses.npz, shape: torch.Size([402, 63])\n",
      "Loaded 05_05_poses.npz, shape: torch.Size([915, 63])\n",
      "Loaded 01_06_poses.npz, shape: torch.Size([5133, 63])\n",
      "Loaded 02_05_poses.npz, shape: torch.Size([1854, 63])\n",
      "Loaded 05_11_poses.npz, shape: torch.Size([591, 63])\n",
      "Loaded 13_27_poses.npz, shape: torch.Size([4169, 63])\n",
      "Loaded 06_12_poses.npz, shape: torch.Size([1222, 63])\n",
      "Loaded 13_42_poses.npz, shape: torch.Size([400, 63])\n",
      "Loaded 13_38_poses.npz, shape: torch.Size([1244, 63])\n",
      "Loaded 13_19_poses.npz, shape: torch.Size([426, 63])\n",
      "Loaded 02_03_poses.npz, shape: torch.Size([173, 63])\n",
      "Loaded 13_35_poses.npz, shape: torch.Size([1201, 63])\n",
      "Loaded 05_03_poses.npz, shape: torch.Size([434, 63])\n",
      "Loaded 05_17_poses.npz, shape: torch.Size([1043, 63])\n",
      "Loaded 06_14_poses.npz, shape: torch.Size([479, 63])\n",
      "Loaded 13_21_poses.npz, shape: torch.Size([5439, 63])\n",
      "Loaded 13_14_poses.npz, shape: torch.Size([2424, 63])\n",
      "Loaded 05_09_poses.npz, shape: torch.Size([1143, 63])\n",
      "Loaded 02_09_poses.npz, shape: torch.Size([1033, 63])\n",
      "Loaded 13_13_poses.npz, shape: torch.Size([439, 63])\n",
      "Loaded 12_01_poses.npz, shape: torch.Size([523, 63])\n",
      "Loaded 13_07_poses.npz, shape: torch.Size([1452, 63])\n",
      "Loaded 02_10_poses.npz, shape: torch.Size([2645, 63])\n",
      "Loaded 05_10_poses.npz, shape: torch.Size([817, 63])\n",
      "Loaded 06_13_poses.npz, shape: torch.Size([4905, 63])\n",
      "Loaded 13_26_poses.npz, shape: torch.Size([3028, 63])\n",
      "Loaded 06_07_poses.npz, shape: torch.Size([576, 63])\n",
      "Loaded 13_32_poses.npz, shape: torch.Size([336, 63])\n",
      "Loaded 05_04_poses.npz, shape: torch.Size([1199, 63])\n",
      "Loaded 01_07_poses.npz, shape: torch.Size([4839, 63])\n",
      "Loaded 02_04_poses.npz, shape: torch.Size([483, 63])\n",
      "Loaded 13_39_poses.npz, shape: torch.Size([351, 63])\n",
      "Loaded 13_18_poses.npz, shape: torch.Size([3000, 63])\n",
      "Loaded 05_16_poses.npz, shape: torch.Size([525, 63])\n",
      "Loaded 13_20_poses.npz, shape: torch.Size([5719, 63])\n",
      "Loaded 06_15_poses.npz, shape: torch.Size([545, 63])\n",
      "Loaded 01_01_poses.npz, shape: torch.Size([2751, 63])\n",
      "Loaded 02_02_poses.npz, shape: torch.Size([298, 63])\n",
      "Loaded 13_34_poses.npz, shape: torch.Size([1417, 63])\n",
      "Loaded 06_01_poses.npz, shape: torch.Size([494, 63])\n",
      "Loaded 05_02_poses.npz, shape: torch.Size([1123, 63])\n",
      "Loaded 13_15_poses.npz, shape: torch.Size([2382, 63])\n",
      "Loaded 13_01_poses.npz, shape: torch.Size([2313, 63])\n",
      "All body poses tensor shape: torch.Size([213368, 63])\n",
      "Saved all body poses to /Users/roawaal/Documents/586Project/latent_poses.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Set your AMASS directory\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')\n",
    "sample_amass_fnames = [\n",
    "    '06_09_poses.npz', '01_09_poses.npz', '13_28_poses.npz', '13_09_poses.npz', '05_07_poses.npz', '13_31_poses.npz',\n",
    "    '06_04_poses.npz', '02_07_poses.npz', '01_10_poses.npz', '13_25_poses.npz', '06_10_poses.npz', '05_13_poses.npz',\n",
    "    '13_04_poses.npz', '12_02_poses.npz', '13_10_poses.npz', '13_40_poses.npz', '05_18_poses.npz', '13_02_poses.npz',\n",
    "    '12_04_poses.npz', '05_20_poses.npz', '13_16_poses.npz', '02_01_poses.npz', '01_02_poses.npz', '05_01_poses.npz',\n",
    "    '13_37_poses.npz', '13_23_poses.npz', '05_15_poses.npz', '13_29_poses.npz', '06_08_poses.npz', '01_08_poses.npz',\n",
    "    '13_08_poses.npz', '01_11_poses.npz', '06_11_poses.npz', '13_24_poses.npz', '05_12_poses.npz', '05_06_poses.npz',\n",
    "    '06_05_poses.npz', '13_30_poses.npz', '02_06_poses.npz', '01_05_poses.npz', '13_11_poses.npz', '13_05_poses.npz',\n",
    "    '12_03_poses.npz', '05_19_poses.npz', '13_41_poses.npz', '13_17_poses.npz', '13_03_poses.npz', '13_22_poses.npz',\n",
    "    '05_14_poses.npz', '01_03_poses.npz', '13_36_poses.npz', '06_03_poses.npz', '05_08_poses.npz', '02_08_poses.npz',\n",
    "    '13_06_poses.npz', '13_12_poses.npz', '13_33_poses.npz', '06_06_poses.npz', '05_05_poses.npz', '01_06_poses.npz',\n",
    "    '02_05_poses.npz', '05_11_poses.npz', '13_27_poses.npz', '06_12_poses.npz', '13_42_poses.npz', '13_38_poses.npz',\n",
    "    '13_19_poses.npz', '02_03_poses.npz', '13_35_poses.npz', '05_03_poses.npz', '05_17_poses.npz', '06_14_poses.npz',\n",
    "    '13_21_poses.npz', '13_14_poses.npz', '05_09_poses.npz', '02_09_poses.npz', '13_13_poses.npz', '12_01_poses.npz',\n",
    "    '13_07_poses.npz', '02_10_poses.npz', '05_10_poses.npz', '06_13_poses.npz', '13_26_poses.npz', '06_07_poses.npz',\n",
    "    '13_32_poses.npz', '05_04_poses.npz', '01_07_poses.npz', '02_04_poses.npz', '13_39_poses.npz', '13_18_poses.npz',\n",
    "    '05_16_poses.npz', '13_20_poses.npz', '06_15_poses.npz', '01_01_poses.npz', '02_02_poses.npz', '13_34_poses.npz',\n",
    "    '06_01_poses.npz', '05_02_poses.npz', '13_15_poses.npz', '13_01_poses.npz'\n",
    "]\n",
    "\n",
    "# Initialize list to store the body poses\n",
    "all_body_poses = []\n",
    "\n",
    "# Iterate through each file in the sample_amass_fnames list\n",
    "for fname in sample_amass_fnames:\n",
    "    file_path = os.path.join(amass_dir, fname)\n",
    "    \n",
    "    # Load the body pose data, excluding global rotation, hands, and fingers (indexing 3:66)\n",
    "    try:\n",
    "        amass_body_pose = np.load(file_path)['poses'][:, 3:66]\n",
    "        amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n",
    "        all_body_poses.append(amass_body_pose)\n",
    "        print(f'Loaded {fname}, shape: {amass_body_pose.shape}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fname}: {e}\")\n",
    "\n",
    "# Convert the list of body poses to a single tensor (if needed)\n",
    "all_body_poses_tensor = torch.cat(all_body_poses, dim=0)\n",
    "\n",
    "# Print the final shape of the concatenated tensor\n",
    "print('All body poses tensor shape:', all_body_poses_tensor.shape)\n",
    "# Save the tensor to a file (you can use .pt or .npz)\n",
    "output_file = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "\n",
    "# Save the tensor to the file\n",
    "torch.save(all_body_poses_tensor, output_file)\n",
    "print(f\"Saved all body poses to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dccc6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9629c452",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Documents/586Project/latent_poses.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2: Load latent pose vectors (ensure shape: [num_samples, latent_dim])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m latent_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Documents/586Project/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m latent_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_poses\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# shape: (N, D)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(latent_poses, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Documents/586Project/latent_poses.npz'"
     ]
    }
   ],
   "source": [
    "# Step 2: Load latent pose vectors (ensure shape: [num_samples, latent_dim])\n",
    "latent_data = np.load('~/Documents/586Project/latent_poses.npz', allow_pickle=True)\n",
    "latent_poses = latent_data['latent_poses']  # shape: (N, D)\n",
    "\n",
    "latent_poses = torch.tensor(latent_poses, dtype=torch.float32).to(device)\n",
    "print('Latent poses shape:', latent_poses.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52fd2fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'latent_poses is not a file in the archive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m latent_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~/Documents/586Project/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m latent_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(latent_path, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m latent_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_poses\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# shape: (N, D)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m latent_poses \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(latent_poses, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatent poses shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, latent_poses\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:260\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a file in the archive\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'latent_poses is not a file in the archive'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 2: Load latent pose vectors (ensure shape: [num_samples, latent_dim])\n",
    "latent_path = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "latent_data = np.load(latent_path, allow_pickle=True)\n",
    "latent_poses = latent_data['latent_poses']  # shape: (N, D)\n",
    "\n",
    "latent_poses = torch.tensor(latent_poses, dtype=torch.float32).to(device)\n",
    "print('Latent poses shape:', latent_poses.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b486ca1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys in the file: ['latent_poses/data.pkl', 'latent_poses/byteorder', 'latent_poses/data/0', 'latent_poses/version', 'latent_poses/.data/serialization_id']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "latent_path = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "latent_data = np.load(latent_path, allow_pickle=True)\n",
    "\n",
    "# Print available keys\n",
    "print(\"Available keys in the file:\", latent_data.files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea60a361",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vposer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     pose_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(pose, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m         posterior \u001b[38;5;241m=\u001b[39m vposer\u001b[38;5;241m.\u001b[39mencode(pose_tensor)\n\u001b[1;32m     10\u001b[0m         latent_means\u001b[38;5;241m.\u001b[39mappend(posterior\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# Only save the mean\u001b[39;00m\n\u001b[1;32m     12\u001b[0m latent_poses_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(latent_means, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape: (N, latent_dim)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vposer' is not defined"
     ]
    }
   ],
   "source": [
    "latent_means = []\n",
    "\n",
    "for fname in npz_files:\n",
    "    data = np.load(os.path.join(amass_dir, fname))\n",
    "    pose = data['poses'][:, 3:66]\n",
    "    pose_tensor = torch.tensor(pose, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        posterior = vposer.encode(pose_tensor)\n",
    "        latent_means.append(posterior.loc.cpu().numpy())  # Only save the mean\n",
    "\n",
    "latent_poses_array = np.concatenate(latent_means, axis=0)  # shape: (N, latent_dim)\n",
    "\n",
    "# Save with a clean key\n",
    "output_path = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "np.savez(output_path, latent_poses=latent_poses_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "073770de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-12 17:58:32.896\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mNo GPU detected. Loading on CPU!\u001b[0m\n",
      "\u001b[32m2025-04-12 17:58:32.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mhuman_body_prior.tools.model_loader\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1mLoaded model in eval mode with trained weights: /Users/roawaal/Documents/586Project/VPoserModelFiles/vposer_v2_05/snapshots/V02_05_epoch=13_val_loss=0.03.ckpt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (213368, 32) latent vectors to /Users/roawaal/Documents/586Project/latent_poses.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from human_body_prior.models.vposer_model import VPoser\n",
    "from human_body_prior.tools.model_loader import load_model\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load VPoser\n",
    "\n",
    "# vposer, _ = load_model(vp_model_path, model_code='vposer')   updated call\n",
    "\n",
    "vp_model_path = os.path.expanduser('~/Documents/586Project/VPoserModelFiles/vposer_v2_05')\n",
    "\n",
    "vposer, _ = load_model(vp_model_path, model_code=VPoser)  # pass the class, not string\n",
    "vposer = vposer.to(device)\n",
    "vposer.eval()\n",
    "# Set AMASS path\n",
    "amass_dir = os.path.expanduser('~/Documents/586Project/AMASS_CMUsubset/')\n",
    "npz_files = sorted([f for f in os.listdir(amass_dir) if f.endswith('.npz')])\n",
    "\n",
    "latent_means = []\n",
    "\n",
    "# Loop through all AMASS files\n",
    "for fname in npz_files:\n",
    "    path = os.path.join(amass_dir, fname)\n",
    "    data = np.load(path)\n",
    "    pose = data['poses'][:, 3:66]  # Remove global rotation and hands\n",
    "\n",
    "    pose_tensor = torch.tensor(pose, dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        posterior = vposer.encode(pose_tensor)\n",
    "        mean = posterior.loc.cpu().numpy()\n",
    "        latent_means.append(mean)\n",
    "\n",
    "# Stack and save\n",
    "latent_poses_array = np.concatenate(latent_means, axis=0)\n",
    "output_path = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "np.savez(output_path, latent_poses=latent_poses_array)\n",
    "print(f\"Saved {latent_poses_array.shape} latent vectors to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5364f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent poses shape: torch.Size([213368, 32])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 2: Load latent pose vectors (ensure shape: [num_samples, latent_dim])\n",
    "latent_path = os.path.expanduser('~/Documents/586Project/latent_poses.npz')\n",
    "latent_data = np.load(latent_path, allow_pickle=True)\n",
    "latent_poses = latent_data['latent_poses']  # shape: (N, D)\n",
    "\n",
    "latent_poses = torch.tensor(latent_poses, dtype=torch.float32).to(device)\n",
    "print('Latent poses shape:', latent_poses.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8caca991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Custom Dataset for sequence modeling\n",
    "class LatentPoseDataset(Dataset):\n",
    "    def __init__(self, data, input_len=10, pred_len=10):\n",
    "        self.data = data\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_len - self.pred_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx:idx+self.input_len]\n",
    "        target_seq = self.data[idx+self.input_len:idx+self.input_len+self.pred_len]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "input_len = 10\n",
    "pred_len = 10\n",
    "dataset = LatentPoseDataset(latent_poses, input_len, pred_len)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b012aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, latent_dim=32, input_len=10, pred_len=10, num_heads=4, hidden_dim=64, num_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=latent_dim, nhead=num_heads, dim_feedforward=hidden_dim),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Linear(latent_dim, latent_dim)  # Output layer to project to latent_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder forward pass\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Decoder: we just apply a final linear transformation to map the output to the latent_dim\n",
    "        output = self.decoder(x)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac9c12ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roawaal/anaconda3/lib/python3.11/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.0008805334923202367\n",
      "Epoch [2/20], Loss: 4.532662542647736e-05\n",
      "Epoch [3/20], Loss: 2.9285504033357947e-05\n",
      "Epoch [4/20], Loss: 2.6944138305204073e-05\n",
      "Epoch [5/20], Loss: 2.611192996862443e-05\n",
      "Epoch [6/20], Loss: 2.567295008559296e-05\n",
      "Epoch [7/20], Loss: 2.5336001781522984e-05\n",
      "Epoch [8/20], Loss: 2.5112108461189373e-05\n",
      "Epoch [9/20], Loss: 2.4906849567612723e-05\n",
      "Epoch [10/20], Loss: 2.4742877827088608e-05\n",
      "Epoch [11/20], Loss: 2.458988558285018e-05\n",
      "Epoch [12/20], Loss: 2.454469722688351e-05\n",
      "Epoch [13/20], Loss: 2.4460182495964212e-05\n",
      "Epoch [14/20], Loss: 2.4358495172477477e-05\n",
      "Epoch [15/20], Loss: 2.429912228312877e-05\n",
      "Epoch [16/20], Loss: 2.4212992037284813e-05\n",
      "Epoch [17/20], Loss: 2.4193012500871984e-05\n",
      "Epoch [18/20], Loss: 2.412707198428672e-05\n",
      "Epoch [19/20], Loss: 2.4051524419243888e-05\n",
      "Epoch [20/20], Loss: 2.4031198447733426e-05\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = TransformerModel(latent_dim=32, input_len=10, pred_len=10, num_heads=4, hidden_dim=64, num_layers=2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for input_seq, target_seq in dataloader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6044411e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted latent vectors: [[-0.07370503  0.01016525  0.02846065 -0.02916978 -0.01136744  0.05673707\n",
      "   0.02509231  0.01817711  0.00714547  0.01445407 -0.04563674  0.01870789\n",
      "  -0.058541   -0.00173027  0.02172298  0.04022223 -0.02616074  0.02114593\n",
      "   0.02125735  0.02603117  0.01280028  0.01042706 -0.00553827 -0.01476279\n",
      "  -0.00874376 -0.06211514 -0.02138592  0.06617448 -0.04979199  0.04204222\n",
      "  -0.00535473 -0.01114113]\n",
      " [-0.07347208  0.01016821  0.02826295 -0.02883393 -0.01214664  0.05669553\n",
      "   0.02498463  0.01855289  0.00737847  0.01476776 -0.0451384   0.01834434\n",
      "  -0.05895503 -0.0015058   0.02155859  0.04003144 -0.02593084  0.02142823\n",
      "   0.02068414  0.02585518  0.01312069  0.01043698 -0.00503035 -0.01379103\n",
      "  -0.00890647 -0.0623561  -0.02050524  0.06609301 -0.04972012  0.04159581\n",
      "  -0.00506371 -0.01157433]\n",
      " [-0.0738547   0.00963577  0.0283133  -0.02835981 -0.01282888  0.05615523\n",
      "   0.02479749  0.01929674  0.00830659  0.01452929 -0.04459688  0.0187166\n",
      "  -0.0588139  -0.00108355  0.02074779  0.0402016  -0.02450694  0.02088591\n",
      "   0.02052329  0.02534384  0.01291641  0.00989755 -0.00493721 -0.01331843\n",
      "  -0.00869174 -0.06227595 -0.01996133  0.06564984 -0.04973861  0.0410211\n",
      "  -0.00484775 -0.01205117]\n",
      " [-0.07426938  0.00992277  0.02840688 -0.02823067 -0.01205991  0.05601032\n",
      "   0.02428684  0.0182688   0.00833053  0.01490861 -0.04473096  0.01857956\n",
      "  -0.05824836 -0.00178808  0.02083879  0.03946832 -0.02570064  0.02050692\n",
      "   0.02072546  0.02585867  0.01321358  0.00985389 -0.00542028 -0.01347744\n",
      "  -0.00913958 -0.06162601 -0.02152796  0.06565761 -0.04947821  0.04166687\n",
      "  -0.00357675 -0.0113877 ]\n",
      " [-0.07432321  0.00981562  0.02805317 -0.02828644 -0.01310403  0.05591853\n",
      "   0.02440756  0.01893815  0.00855141  0.01476587 -0.04479825  0.01812293\n",
      "  -0.05895432 -0.00166444  0.02123458  0.03974303 -0.02510815  0.02053422\n",
      "   0.02023873  0.02564802  0.01328425  0.00951499 -0.00468722 -0.01288437\n",
      "  -0.00882267 -0.06199471 -0.02052029  0.06604612 -0.04923252  0.04143982\n",
      "  -0.00444159 -0.01162944]\n",
      " [-0.07447395  0.00974635  0.02837008 -0.02814358 -0.01438835  0.0554852\n",
      "   0.02487422  0.01940719  0.00836254  0.01405071 -0.04539219  0.0178094\n",
      "  -0.05927303 -0.00154807  0.02165871  0.04023917 -0.02394754  0.02091496\n",
      "   0.01993691  0.02563044  0.01292774  0.00965102 -0.00380528 -0.01294296\n",
      "  -0.0081673  -0.06243935 -0.01926558  0.06598573 -0.0498029   0.04095337\n",
      "  -0.00568674 -0.01185014]\n",
      " [-0.07402454  0.01006381  0.02860289 -0.028012   -0.01420302  0.05570683\n",
      "   0.02471307  0.01907057  0.00791278  0.01469009 -0.04573042  0.01763889\n",
      "  -0.05956758 -0.00187108  0.02134805  0.03989521 -0.02528863  0.02092407\n",
      "   0.0200402   0.02576859  0.01283704  0.00981746 -0.00365725 -0.01311697\n",
      "  -0.0082222  -0.06191361 -0.01950695  0.06650972 -0.04946737  0.04125291\n",
      "  -0.00543457 -0.01143862]\n",
      " [-0.07421084  0.01028378  0.02879912 -0.02815166 -0.01424439  0.05540176\n",
      "   0.02492858  0.01905695  0.0071452   0.01435366 -0.04635634  0.01782904\n",
      "  -0.05936384 -0.00206706  0.02191121  0.03983676 -0.02593329  0.02035582\n",
      "   0.02049794  0.02583513  0.01297829  0.00974932 -0.0036489  -0.01379016\n",
      "  -0.00781132 -0.06130359 -0.02035606  0.06696321 -0.04982318  0.04155018\n",
      "  -0.00505587 -0.01111228]\n",
      " [-0.07378443  0.01085965  0.02874359 -0.02816227 -0.014177    0.05557168\n",
      "   0.02510288  0.01878352  0.00691764  0.01502009 -0.0463521   0.01762368\n",
      "  -0.05986529 -0.00224674  0.02225313  0.03926234 -0.02716179  0.02025313\n",
      "   0.02055577  0.02592204  0.01351591  0.01013951 -0.00370685 -0.01331362\n",
      "  -0.00785884 -0.0610719  -0.02070069  0.06743139 -0.04950713  0.04170757\n",
      "  -0.00447218 -0.01097244]\n",
      " [-0.07345904  0.0112147   0.02869469 -0.02799801 -0.01377223  0.05586823\n",
      "   0.0250488   0.01829624  0.00681735  0.01581891 -0.04623425  0.01776771\n",
      "  -0.06003476 -0.00222849  0.02220234  0.03880198 -0.02832691  0.02039597\n",
      "   0.02065897  0.02606994  0.01385528  0.0104337  -0.00396588 -0.01313028\n",
      "  -0.00803237 -0.06073877 -0.02111557  0.06742652 -0.04924604  0.04186316\n",
      "  -0.0035075  -0.01116938]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/22/cyt666h549z5wdq3v47q2hqm0000gn/T/ipykernel_7746/2126993976.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_input_seq = torch.tensor(test_input_seq, dtype=torch.float32).unsqueeze(0).to(device)  # Add batch dimension\n"
     ]
    }
   ],
   "source": [
    "# Select a test sequence from the dataset (this could be from the test set or a sample)\n",
    "test_input_seq = latent_poses[1000:1010]  # Example: first 10 frames from the data\n",
    "test_input_seq = torch.tensor(test_input_seq, dtype=torch.float32).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "# Use the model to predict future latent vectors\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predicted_output = model(test_input_seq)\n",
    "\n",
    "# Convert the predicted output back to CPU for further analysis or visualization\n",
    "predicted_output = predicted_output.squeeze(0).cpu().numpy()  # Remove batch dimension\n",
    "\n",
    "print(f\"Predicted latent vectors: {predicted_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9d7cfa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted latent vectors: [[-0.07370503  0.01016525  0.02846065 -0.02916978 -0.01136744  0.05673707\n",
      "   0.02509231  0.01817711  0.00714547  0.01445407 -0.04563674  0.01870789\n",
      "  -0.058541   -0.00173027  0.02172298  0.04022223 -0.02616074  0.02114593\n",
      "   0.02125735  0.02603117  0.01280028  0.01042706 -0.00553827 -0.01476279\n",
      "  -0.00874376 -0.06211514 -0.02138592  0.06617448 -0.04979199  0.04204222\n",
      "  -0.00535473 -0.01114113]\n",
      " [-0.07347208  0.01016821  0.02826295 -0.02883393 -0.01214664  0.05669553\n",
      "   0.02498463  0.01855289  0.00737847  0.01476776 -0.0451384   0.01834434\n",
      "  -0.05895503 -0.0015058   0.02155859  0.04003144 -0.02593084  0.02142823\n",
      "   0.02068414  0.02585518  0.01312069  0.01043698 -0.00503035 -0.01379103\n",
      "  -0.00890647 -0.0623561  -0.02050524  0.06609301 -0.04972012  0.04159581\n",
      "  -0.00506371 -0.01157433]\n",
      " [-0.0738547   0.00963577  0.0283133  -0.02835981 -0.01282888  0.05615523\n",
      "   0.02479749  0.01929674  0.00830659  0.01452929 -0.04459688  0.0187166\n",
      "  -0.0588139  -0.00108355  0.02074779  0.0402016  -0.02450694  0.02088591\n",
      "   0.02052329  0.02534384  0.01291641  0.00989755 -0.00493721 -0.01331843\n",
      "  -0.00869174 -0.06227595 -0.01996133  0.06564984 -0.04973861  0.0410211\n",
      "  -0.00484775 -0.01205117]\n",
      " [-0.07426938  0.00992277  0.02840688 -0.02823067 -0.01205991  0.05601032\n",
      "   0.02428684  0.0182688   0.00833053  0.01490861 -0.04473096  0.01857956\n",
      "  -0.05824836 -0.00178808  0.02083879  0.03946832 -0.02570064  0.02050692\n",
      "   0.02072546  0.02585867  0.01321358  0.00985389 -0.00542028 -0.01347744\n",
      "  -0.00913958 -0.06162601 -0.02152796  0.06565761 -0.04947821  0.04166687\n",
      "  -0.00357675 -0.0113877 ]\n",
      " [-0.07432321  0.00981562  0.02805317 -0.02828644 -0.01310403  0.05591853\n",
      "   0.02440756  0.01893815  0.00855141  0.01476587 -0.04479825  0.01812293\n",
      "  -0.05895432 -0.00166444  0.02123458  0.03974303 -0.02510815  0.02053422\n",
      "   0.02023873  0.02564802  0.01328425  0.00951499 -0.00468722 -0.01288437\n",
      "  -0.00882267 -0.06199471 -0.02052029  0.06604612 -0.04923252  0.04143982\n",
      "  -0.00444159 -0.01162944]\n",
      " [-0.07447395  0.00974635  0.02837008 -0.02814358 -0.01438835  0.0554852\n",
      "   0.02487422  0.01940719  0.00836254  0.01405071 -0.04539219  0.0178094\n",
      "  -0.05927303 -0.00154807  0.02165871  0.04023917 -0.02394754  0.02091496\n",
      "   0.01993691  0.02563044  0.01292774  0.00965102 -0.00380528 -0.01294296\n",
      "  -0.0081673  -0.06243935 -0.01926558  0.06598573 -0.0498029   0.04095337\n",
      "  -0.00568674 -0.01185014]\n",
      " [-0.07402454  0.01006381  0.02860289 -0.028012   -0.01420302  0.05570683\n",
      "   0.02471307  0.01907057  0.00791278  0.01469009 -0.04573042  0.01763889\n",
      "  -0.05956758 -0.00187108  0.02134805  0.03989521 -0.02528863  0.02092407\n",
      "   0.0200402   0.02576859  0.01283704  0.00981746 -0.00365725 -0.01311697\n",
      "  -0.0082222  -0.06191361 -0.01950695  0.06650972 -0.04946737  0.04125291\n",
      "  -0.00543457 -0.01143862]\n",
      " [-0.07421084  0.01028378  0.02879912 -0.02815166 -0.01424439  0.05540176\n",
      "   0.02492858  0.01905695  0.0071452   0.01435366 -0.04635634  0.01782904\n",
      "  -0.05936384 -0.00206706  0.02191121  0.03983676 -0.02593329  0.02035582\n",
      "   0.02049794  0.02583513  0.01297829  0.00974932 -0.0036489  -0.01379016\n",
      "  -0.00781132 -0.06130359 -0.02035606  0.06696321 -0.04982318  0.04155018\n",
      "  -0.00505587 -0.01111228]\n",
      " [-0.07378443  0.01085965  0.02874359 -0.02816227 -0.014177    0.05557168\n",
      "   0.02510288  0.01878352  0.00691764  0.01502009 -0.0463521   0.01762368\n",
      "  -0.05986529 -0.00224674  0.02225313  0.03926234 -0.02716179  0.02025313\n",
      "   0.02055577  0.02592204  0.01351591  0.01013951 -0.00370685 -0.01331362\n",
      "  -0.00785884 -0.0610719  -0.02070069  0.06743139 -0.04950713  0.04170757\n",
      "  -0.00447218 -0.01097244]\n",
      " [-0.07345904  0.0112147   0.02869469 -0.02799801 -0.01377223  0.05586823\n",
      "   0.0250488   0.01829624  0.00681735  0.01581891 -0.04623425  0.01776771\n",
      "  -0.06003476 -0.00222849  0.02220234  0.03880198 -0.02832691  0.02039597\n",
      "   0.02065897  0.02606994  0.01385528  0.0104337  -0.00396588 -0.01313028\n",
      "  -0.00803237 -0.06073877 -0.02111557  0.06742652 -0.04924604  0.04186316\n",
      "  -0.0035075  -0.01116938]]\n"
     ]
    }
   ],
   "source": [
    "# Select a test sequence from the dataset (this could be from the test set or a sample)\n",
    "test_input_seq = latent_poses[1000:1010]  # Example: first 10 frames from the data\n",
    "test_input_seq = test_input_seq.clone().detach().unsqueeze(0).to(device)  # Add batch dimension, avoid gradients\n",
    "\n",
    "# Use the model to predict future latent vectors\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predicted_output = model(test_input_seq)\n",
    "\n",
    "# Convert the predicted output back to CPU for further analysis or visualization\n",
    "predicted_output = predicted_output.squeeze(0).cpu().numpy()  # Remove batch dimension\n",
    "\n",
    "print(f\"Predicted latent vectors: {predicted_output}\")\n",
    "# Save the predicted latent vectors into a .npz file\n",
    "output_file = os.path.expanduser('~/Documents/586Project/predicted_latent_vectors.npz')\n",
    "np.savez(output_file, predicted_latent_vectors=predicted_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04d0f231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3146c432",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming vp is the VPoser model and amass_body_pose is the tensor of latent vectors\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Decode the latent vectors back to body poses\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m decoded_poses \u001b[38;5;241m=\u001b[39m vp\u001b[38;5;241m.\u001b[39mdecode(amass_body_pose)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract the body pose from the decoder output (ensure the key matches your model's output)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m amass_body_pose_rec \u001b[38;5;241m=\u001b[39m decoded_poses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m63\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/models/vposer_model.py:101\u001b[0m, in \u001b[0;36mVPoser.decode\u001b[0;34m(self, Zin)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, Zin):\n\u001b[1;32m     99\u001b[0m     bs \u001b[38;5;241m=\u001b[39m Zin\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 101\u001b[0m     prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_net(Zin)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m: matrot2aa(prec\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body_matrot\u001b[39m\u001b[38;5;124m'\u001b[39m: prec\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[1;32m    106\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x512)"
     ]
    }
   ],
   "source": [
    "# Assuming vp is the VPoser model and amass_body_pose is the tensor of latent vectors\n",
    "# Decode the latent vectors back to body poses\n",
    "decoded_poses = vp.decode(amass_body_pose)\n",
    "\n",
    "# Extract the body pose from the decoder output (ensure the key matches your model's output)\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "# Check the shape of the reconstructed poses\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "424df279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2313, 63])\n"
     ]
    }
   ],
   "source": [
    "print(amass_body_pose.shape)  # Check the shape of the input tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d772b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of expanding latent vector to match expected dimension (if needed)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m latent_expansion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Expanding from 32 to 512\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m expanded_latent \u001b[38;5;241m=\u001b[39m latent_expansion(amass_body_pose)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now, decode using the expanded latent vector\u001b[39;00m\n\u001b[1;32m      6\u001b[0m decoded_poses \u001b[38;5;241m=\u001b[39m vp\u001b[38;5;241m.\u001b[39mdecode(expanded_latent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x512)"
     ]
    }
   ],
   "source": [
    "# Example of expanding latent vector to match expected dimension (if needed)\n",
    "latent_expansion = nn.Linear(32, 512).to(device)  # Expanding from 32 to 512\n",
    "expanded_latent = latent_expansion(amass_body_pose)\n",
    "\n",
    "# Now, decode using the expanded latent vector\n",
    "decoded_poses = vp.decode(expanded_latent)\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ff65e68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x2313)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example of expanding latent vector to match expected dimension (if needed)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m latent_expansion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m2313\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Expanding from 32 to 512\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m expanded_latent \u001b[38;5;241m=\u001b[39m latent_expansion(amass_body_pose)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now, decode using the expanded latent vector\u001b[39;00m\n\u001b[1;32m      6\u001b[0m decoded_poses \u001b[38;5;241m=\u001b[39m vp\u001b[38;5;241m.\u001b[39mdecode(expanded_latent)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2313x63 and 32x2313)"
     ]
    }
   ],
   "source": [
    "# Example of expanding latent vector to match expected dimension (if needed)\n",
    "latent_expansion = nn.Linear(32, 2313).to(device)  # Expanding from 32 to 512\n",
    "expanded_latent = latent_expansion(amass_body_pose)\n",
    "\n",
    "# Now, decode using the expanded latent vector\n",
    "decoded_poses = vp.decode(expanded_latent)\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42a277ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x512 and 32x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m expanded_latent \u001b[38;5;241m=\u001b[39m latent_expansion(amass_body_pose)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Decode the expanded latent vector using VPoser\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m decoded_poses \u001b[38;5;241m=\u001b[39m vposer\u001b[38;5;241m.\u001b[39mdecode(expanded_latent)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract the body pose from the decoder output\u001b[39;00m\n\u001b[1;32m     14\u001b[0m amass_body_pose_rec \u001b[38;5;241m=\u001b[39m decoded_poses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m63\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/models/vposer_model.py:101\u001b[0m, in \u001b[0;36mVPoser.decode\u001b[0;34m(self, Zin)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, Zin):\n\u001b[1;32m     99\u001b[0m     bs \u001b[38;5;241m=\u001b[39m Zin\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 101\u001b[0m     prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_net(Zin)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m: matrot2aa(prec\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body_matrot\u001b[39m\u001b[38;5;124m'\u001b[39m: prec\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m9\u001b[39m)\n\u001b[1;32m    106\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x512 and 32x512)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example input: latent vector of size (N, 32)\n",
    "# Let's assume 'amass_body_pose' is your latent vector of shape (N, 32)\n",
    "# Replace this with the actual latent vector you're working with\n",
    "amass_body_pose = torch.randn(1, 63).to(device)  # Random tensor for demonstration\n",
    "\n",
    "# Expand the latent vector (from size 32 to 512)\n",
    "latent_expansion = nn.Linear(63, 512).to(device)  # Expanding from 32 to 512\n",
    "expanded_latent = latent_expansion(amass_body_pose)\n",
    "\n",
    "# Decode the expanded latent vector using VPoser\n",
    "decoded_poses = vposer.decode(expanded_latent)\n",
    "\n",
    "# Extract the body pose from the decoder output\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "# Print the shape of the decoded body poses\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20336f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amass_body_pose_rec.shape torch.Size([1, 63])\n"
     ]
    }
   ],
   "source": [
    "amass_body_pose = torch.randn(1, 512).to(device)  # Latent vector of size (1, 512)\n",
    "\n",
    "# Adjust the input latent vector to (1, 32) before feeding into VPoser if required\n",
    "latent_adjustment = nn.Linear(512, 32).to(device)  # Adjusting from 512 to 32\n",
    "adjusted_latent = latent_adjustment(amass_body_pose)\n",
    "\n",
    "# Decode the adjusted latent vector using VPoser\n",
    "decoded_poses = vposer.decode(adjusted_latent)\n",
    "\n",
    "# Extract the body pose from the decoder output\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "# Print the shape of the decoded body poses\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "200e16fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3]' is invalid for input of size 614",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m originalPoses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m:amass_body_pose}\n\u001b[1;32m      5\u001b[0m recoveredPoses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m:amass_body_pose_rec}\n\u001b[0;32m----> 7\u001b[0m bmodelorig \u001b[38;5;241m=\u001b[39m bm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moriginalPoses);\n\u001b[1;32m      8\u001b[0m bmodelreco \u001b[38;5;241m=\u001b[39m bm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrecoveredPoses);\n\u001b[1;32m      9\u001b[0m vorig \u001b[38;5;241m=\u001b[39m c2c(bmodelorig\u001b[38;5;241m.\u001b[39mv)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/body_model/body_model.py:257\u001b[0m, in \u001b[0;36mBodyModel.forward\u001b[0;34m(self, root_orient, pose_body, pose_hand, pose_jaw, pose_eye, betas, trans, dmpls, expression, v_template, joints, v_shaped, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     shape_components \u001b[38;5;241m=\u001b[39m betas\n\u001b[1;32m    255\u001b[0m     shapedirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapedirs\n\u001b[0;32m--> 257\u001b[0m verts, Jtr \u001b[38;5;241m=\u001b[39m lbs(betas\u001b[38;5;241m=\u001b[39mshape_components, pose\u001b[38;5;241m=\u001b[39mfull_pose, v_template\u001b[38;5;241m=\u001b[39mv_template,\n\u001b[1;32m    258\u001b[0m                     shapedirs\u001b[38;5;241m=\u001b[39mshapedirs, posedirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposedirs,\n\u001b[1;32m    259\u001b[0m                     J_regressor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_regressor, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkintree_table[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    260\u001b[0m                     lbs_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, joints\u001b[38;5;241m=\u001b[39mjoints, v_shaped\u001b[38;5;241m=\u001b[39mv_shaped,\n\u001b[1;32m    261\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    263\u001b[0m Jtr \u001b[38;5;241m=\u001b[39m Jtr \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m verts \u001b[38;5;241m=\u001b[39m verts \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/body_model/lbs.py:223\u001b[0m, in \u001b[0;36mlbs\u001b[0;34m(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents, lbs_weights, joints, pose2rot, v_shaped, dtype)\u001b[0m\n\u001b[1;32m    220\u001b[0m ident \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pose2rot:\n\u001b[1;32m    222\u001b[0m     rot_mats \u001b[38;5;241m=\u001b[39m batch_rodrigues(\n\u001b[0;32m--> 223\u001b[0m         pose\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    225\u001b[0m     pose_feature \u001b[38;5;241m=\u001b[39m (rot_mats[:, \u001b[38;5;241m1\u001b[39m:, :, :] \u001b[38;5;241m-\u001b[39m ident)\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# (N x P) x (P, V * 3) -> N x V x 3\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3]' is invalid for input of size 614"
     ]
    }
   ],
   "source": [
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "import trimesh\n",
    "\n",
    "originalPoses = {'pose_body':amass_body_pose}\n",
    "recoveredPoses = {'pose_body':amass_body_pose_rec}\n",
    "\n",
    "bmodelorig = bm(**originalPoses);\n",
    "bmodelreco = bm(**recoveredPoses);\n",
    "vorig = c2c(bmodelorig.v)\n",
    "vreco = c2c(bmodelreco.v)\n",
    "faces = c2c(bm.f)\n",
    "\n",
    "T, num_verts = vorig.shape[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9fe7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Loss (MSE): 0.9548721313476562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roawaal/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 63])) that is different to the input size (torch.Size([10, 63])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Test the Model on an Input Sequence\n",
    "test_input_seq = torch.randn(1, 10, 32).to(device)  # Example test sequence\n",
    "predicted_latent_poses = model(test_input_seq)\n",
    "\n",
    "# Decode predicted latent poses\n",
    "decoded_poses = vp.decode(predicted_latent_poses)\n",
    "predicted_smpl_poses = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "# Step 7: Compare with Actual Ground Truth Poses (use MSE)\n",
    "actual_smpl_poses = torch.randn(1, 63).to(device)  # Example ground truth, replace with actual data\n",
    "loss = nn.MSELoss()(predicted_smpl_poses, actual_smpl_poses)\n",
    "print(\"Prediction Loss (MSE):\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb561188",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert predicted output to numpy and visualize\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m predicted_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example: Convert predicted latent vector back to body pose (e.g., SMPL)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Assuming `vposer.decode` can take these latent vectors (change to match your model)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m predicted_poses \u001b[38;5;241m=\u001b[39m vposer\u001b[38;5;241m.\u001b[39mdecode(torch\u001b[38;5;241m.\u001b[39mtensor(predicted_output)\u001b[38;5;241m.\u001b[39mto(device))  \u001b[38;5;66;03m# Decode the latent vectors to SMPL poses\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import trimesh\n",
    "import os\n",
    "\n",
    "# Convert predicted output to numpy and visualize\n",
    "predicted_output = predicted_output.squeeze(0).cpu().numpy()  # Remove batch dimension\n",
    "\n",
    "# Example: Convert predicted latent vector back to body pose (e.g., SMPL)\n",
    "# Assuming `vposer.decode` can take these latent vectors (change to match your model)\n",
    "predicted_poses = vposer.decode(torch.tensor(predicted_output).to(device))  # Decode the latent vectors to SMPL poses\n",
    "predicted_body_pose = predicted_poses['pose_body'].cpu().numpy()\n",
    "\n",
    "print(f\"Predicted latent vectors: {predicted_output}\")\n",
    "print(f\"Predicted body pose: {predicted_body_pose}\")\n",
    "\n",
    "# Visualize a frame of the predicted body pose (assuming you want to visualize the first frame)\n",
    "fIdx = 0\n",
    "verts = predicted_body_pose[fIdx]  # Get the predicted body pose (vertices)\n",
    "faces = smpl_model.faces  # Assuming you have access to the SMPL model faces (this may vary)\n",
    "mesh_pred = trimesh.base.Trimesh(verts, faces)\n",
    "mesh_pred.visual.vertex_colors = [254, 66, 200]  # Set color to purple for predicted pose\n",
    "\n",
    "# You may also want to visualize the original (actual) pose for comparison\n",
    "# Assuming you have the ground truth pose (actual pose) as 'actual_body_pose'\n",
    "actual_body_pose = np.load('path_to_ground_truth_data.npz')['body_pose']  # Replace with actual path\n",
    "verts_actual = actual_body_pose[fIdx]\n",
    "mesh_actual = trimesh.base.Trimesh(verts_actual, faces)\n",
    "mesh_actual.visual.vertex_colors = [254, 254, 254]  # Set color to grey for actual pose\n",
    "\n",
    "# Apply translation to overlay meshes (optional)\n",
    "mesh_pred.apply_translation([1, 0, 0])  # Offset predicted pose for visualization\n",
    "\n",
    "# Create a scene with both meshes (actual and predicted poses)\n",
    "meshes = [mesh_actual, mesh_pred]\n",
    "trimesh.Scene(meshes).show()\n",
    "\n",
    "# Save the predicted latent vectors into a .npz file\n",
    "output_file = os.path.expanduser('~/Documents/586Project/predicted_latent_vectors.npz')\n",
    "np.savez(output_file, predicted_latent_vectors=predicted_output)\n",
    "\n",
    "# Optionally, save the predicted body poses if you want to visualize later\n",
    "predicted_body_pose_file = os.path.expanduser('~/Documents/586Project/predicted_body_poses.npz')\n",
    "np.savez(predicted_body_pose_file, predicted_body_pose=predicted_body_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6462528f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Documents/586Project/latent_poses.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m fIdx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m140\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example vertices (replace with actual data arrays)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# These are placeholders for the actual vertices; replace them with your data.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 'vorig' should be the original (actual) body pose vertices and 'vreco' should be the predicted (reconstructed) body pose vertices.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m vorig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Documents/586Project/latent_poses.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody_pose\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Load original (ground truth) poses\u001b[39;00m\n\u001b[1;32m     10\u001b[0m vreco \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Documents/586Project/predicted_latent_vectors.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_body_pose\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Load predicted (reconstructed) poses\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Get the vertices for the frame 'fIdx'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Documents/586Project/latent_poses.npz'"
     ]
    }
   ],
   "source": [
    "import trimesh\n",
    "\n",
    "# Example frame index (e.g., fIdx = 140)\n",
    "fIdx = 140\n",
    "\n",
    "# Example vertices (replace with actual data arrays)\n",
    "# These are placeholders for the actual vertices; replace them with your data.\n",
    "# 'vorig' should be the original (actual) body pose vertices and 'vreco' should be the predicted (reconstructed) body pose vertices.\n",
    "vorig = np.load('/Documents/586Project/latent_poses.npz')['body_pose']  # Load original (ground truth) poses\n",
    "vreco = np.load('/Documents/586Project/predicted_latent_vectors.npz')['predicted_body_pose']  # Load predicted (reconstructed) poses\n",
    "\n",
    "# Get the vertices for the frame 'fIdx'\n",
    "verts_original = vorig[fIdx]  # Actual body pose\n",
    "verts_reconstructed = vreco[fIdx]  # Predicted body pose\n",
    "\n",
    "# Assuming the mesh faces are available (replace with the correct faces of your model)\n",
    "faces = smpl_model.faces  # Assuming you have access to the SMPL faces (replace as needed)\n",
    "\n",
    "# Create mesh for the original body pose (before encoding)\n",
    "mesh1 = trimesh.base.Trimesh(verts_original, faces)\n",
    "mesh1.visual.vertex_colors = [254, 254, 254]  # Set color to grey for actual pose\n",
    "\n",
    "# Create mesh for the predicted body pose (after decoding)\n",
    "mesh2 = trimesh.base.Trimesh(verts_reconstructed, faces)\n",
    "mesh2.visual.vertex_colors = [254, 66, 200]  # Set color to purple for predicted pose\n",
    "\n",
    "# Optionally, translate the predicted mesh to better visualize both (overlay)\n",
    "mesh2.apply_translation([1, 0, 0])  # Translate mesh2 along x-axis (or choose another offset)\n",
    "\n",
    "# Create a scene with both meshes (original and reconstructed poses)\n",
    "meshes = [mesh1, mesh2]\n",
    "\n",
    "# Visualize the meshes in a 3D scene\n",
    "trimesh.Scene(meshes).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34ece645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amass_body_pose_rec.shape torch.Size([1, 63])\n",
      "Decoded body poses saved to: /Users/roawaal/Documents/586Project/decoded_body_poses.npz\n"
     ]
    }
   ],
   "source": [
    "amass_body_pose = torch.randn(1, 512).to(device)  # Latent vector of size (1, 512)\n",
    "\n",
    "# Adjust the input latent vector to (1, 32) before feeding into VPoser if required\n",
    "latent_adjustment = nn.Linear(512, 32).to(device)  # Adjusting from 512 to 32\n",
    "adjusted_latent = latent_adjustment(amass_body_pose)\n",
    "\n",
    "# Decode the adjusted latent vector using VPoser\n",
    "decoded_poses = vposer.decode(adjusted_latent)\n",
    "\n",
    "# Extract the body pose from the decoder output\n",
    "amass_body_pose_rec = decoded_poses['pose_body'].contiguous().view(-1, 63)\n",
    "\n",
    "\n",
    "# Print the shape of the decoded body poses\n",
    "print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)\n",
    "\n",
    "# Convert to CPU for saving to file (if it's on GPU)\n",
    "amass_body_pose_rec = amass_body_pose_rec.cpu().detach().numpy()\n",
    "\n",
    "# Save the decoded body poses to a .npz file\n",
    "output_file = os.path.expanduser('~/Documents/586Project/decoded_body_poses.npz')\n",
    "np.savez(output_file, decoded_body_poses=amass_body_pose_rec)\n",
    "\n",
    "print(f\"Decoded body poses saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbc8ce1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'smplx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msmplx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMPL\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load original and reconstructed pose_body sequences (63D axis-angle)\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'smplx'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from smplx import SMPL\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load original and reconstructed pose_body sequences (63D axis-angle)\n",
    "original_data = np.load('latent_poses.npz')['pose_body']         # Shape: (N, 63)\n",
    "decoded_data = np.load('decoded_body_poses.npz')['pose_body']   # Shape: (N, 63)\n",
    "\n",
    "# Convert to torch tensors\n",
    "pose_body_orig = torch.tensor(original_data, dtype=torch.float32).to(device)\n",
    "pose_body_reco = torch.tensor(decoded_data, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a5e8c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smplx\n",
      "  Obtaining dependency information for smplx from https://files.pythonhosted.org/packages/c9/33/bd37416aec828e7465652d9e2525fe52de0a29a581f1519cc51a74485091/smplx-0.1.28-py3-none-any.whl.metadata\n",
      "  Downloading smplx-0.1.28-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from smplx) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.0.1.post2 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from smplx) (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from torch>=1.0.1.post2->smplx) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.0.1.post2->smplx) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/roawaal/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.0.1.post2->smplx) (1.3.0)\n",
      "Downloading smplx-0.1.28-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: smplx\n",
      "Successfully installed smplx-0.1.28\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install smplx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0073be34",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 3]' is invalid for input of size 614",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m recoveredPoses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpose_body\u001b[39m\u001b[38;5;124m'\u001b[39m: amass_body_pose_rec}   \u001b[38;5;66;03m# Reconstructed SMPL pose\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the mesh models from SMPL with the respective poses\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m bmodelorig \u001b[38;5;241m=\u001b[39m bm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moriginalPoses)   \u001b[38;5;66;03m# original mesh\u001b[39;00m\n\u001b[1;32m     10\u001b[0m bmodelreco \u001b[38;5;241m=\u001b[39m bm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrecoveredPoses)  \u001b[38;5;66;03m# reconstructed mesh\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Extract vertices (v) and convert to CPU numpy arrays\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/body_model/body_model.py:257\u001b[0m, in \u001b[0;36mBodyModel.forward\u001b[0;34m(self, root_orient, pose_body, pose_hand, pose_jaw, pose_eye, betas, trans, dmpls, expression, v_template, joints, v_shaped, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     shape_components \u001b[38;5;241m=\u001b[39m betas\n\u001b[1;32m    255\u001b[0m     shapedirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapedirs\n\u001b[0;32m--> 257\u001b[0m verts, Jtr \u001b[38;5;241m=\u001b[39m lbs(betas\u001b[38;5;241m=\u001b[39mshape_components, pose\u001b[38;5;241m=\u001b[39mfull_pose, v_template\u001b[38;5;241m=\u001b[39mv_template,\n\u001b[1;32m    258\u001b[0m                     shapedirs\u001b[38;5;241m=\u001b[39mshapedirs, posedirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposedirs,\n\u001b[1;32m    259\u001b[0m                     J_regressor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mJ_regressor, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkintree_table[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong(),\n\u001b[1;32m    260\u001b[0m                     lbs_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, joints\u001b[38;5;241m=\u001b[39mjoints, v_shaped\u001b[38;5;241m=\u001b[39mv_shaped,\n\u001b[1;32m    261\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    263\u001b[0m Jtr \u001b[38;5;241m=\u001b[39m Jtr \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m verts \u001b[38;5;241m=\u001b[39m verts \u001b[38;5;241m+\u001b[39m trans\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/human_body_prior/body_model/lbs.py:223\u001b[0m, in \u001b[0;36mlbs\u001b[0;34m(betas, pose, v_template, shapedirs, posedirs, J_regressor, parents, lbs_weights, joints, pose2rot, v_shaped, dtype)\u001b[0m\n\u001b[1;32m    220\u001b[0m ident \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;241m3\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pose2rot:\n\u001b[1;32m    222\u001b[0m     rot_mats \u001b[38;5;241m=\u001b[39m batch_rodrigues(\n\u001b[0;32m--> 223\u001b[0m         pose\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    225\u001b[0m     pose_feature \u001b[38;5;241m=\u001b[39m (rot_mats[:, \u001b[38;5;241m1\u001b[39m:, :, :] \u001b[38;5;241m-\u001b[39m ident)\u001b[38;5;241m.\u001b[39mview([batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# (N x P) x (P, V * 3) -> N x V x 3\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 3]' is invalid for input of size 614"
     ]
    }
   ],
   "source": [
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "import trimesh\n",
    "\n",
    "# Create pose dictionaries\n",
    "originalPoses = {'pose_body': amass_body_pose}        # SMPL pose vector (axis-angle), shape: (T, 63)\n",
    "recoveredPoses = {'pose_body': amass_body_pose_rec}   # Reconstructed SMPL pose\n",
    "\n",
    "# Get the mesh models from SMPL with the respective poses\n",
    "bmodelorig = bm(**originalPoses)   # original mesh\n",
    "bmodelreco = bm(**recoveredPoses)  # reconstructed mesh\n",
    "\n",
    "# Extract vertices (v) and convert to CPU numpy arrays\n",
    "vorig = c2c(bmodelorig.v)  # shape: (T, 10475, 3)\n",
    "vreco = c2c(bmodelreco.v)  # shape: (T, 10475, 3)\n",
    "\n",
    "# Extract faces (f) from the body model (static across poses)\n",
    "faces = c2c(bm.f)          # shape: (20832, 3), triangle mesh\n",
    "\n",
    "# Get number of time steps and number of vertices\n",
    "T, num_verts = vorig.shape[:-1]\n",
    "print(f\"Number of frames: {T}, Number of vertices per mesh: {num_verts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbf442e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 63]' is invalid for input of size 512",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m amass_body_pose \u001b[38;5;241m=\u001b[39m amass_body_pose\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m63\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 63]' is invalid for input of size 512"
     ]
    }
   ],
   "source": [
    "amass_body_pose = amass_body_pose.view(1, 63)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7456202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
